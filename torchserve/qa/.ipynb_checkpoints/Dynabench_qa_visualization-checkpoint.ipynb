{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (3.2.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: seaborn in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from seaborn) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.2->seaborn) (1.15.0)\n",
      "Requirement already satisfied: pandas in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: transformers in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: numpy in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: joblib in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: captum in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from captum) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from captum) (3.2.2)\n",
      "Requirement already satisfied: torch>=1.2 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from captum) (1.5.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib->captum) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib->captum) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib->captum) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from matplotlib->captum) (0.10.0)\n",
      "Requirement already satisfied: future in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from torch>=1.2->captum) (0.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hamidnazeri/opt/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded\n",
      "Question:  What is important to us?\n",
      "Predicted Answer:  Ä important\n",
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>10</b></text></td><td><text style=\"padding-right:2em\"><b>10 (0.54)</b></text></td><td><text style=\"padding-right:2em\"><b>13</b></text></td><td><text style=\"padding-right:2em\"><b>2.04</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä empower                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For End Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>10</b></text></td><td><text style=\"padding-right:2em\"><b>10 (0.36)</b></text></td><td><text style=\"padding-right:2em\"><b>22</b></text></td><td><text style=\"padding-right:2em\"><b>1.86</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä empower                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Ä kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #/s                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoConfig\n",
    "from captum_utils import *\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = './qa'\n",
    "\n",
    "# load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"model is loaded\")\n",
    "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "def predict(inputs, position_ids=None, attention_mask=None):\n",
    "    return model(inputs, position_ids=position_ids, attention_mask=attention_mask, )\n",
    "\n",
    "def squad_pos_forward_func(inputs,position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "   \n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_bert_sub_embedding(input_ids, ref_input_ids,\n",
    "                                   position_ids, ref_position_ids):\n",
    "    input_embeddings = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding1.indices_to_embeddings(ref_input_ids)\n",
    "\n",
    "    input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(position_ids)\n",
    "    ref_input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(ref_position_ids)\n",
    "\n",
    "    return (input_embeddings, ref_input_embeddings), \\\n",
    "           (input_embeddings_position_ids, ref_input_embeddings_position_ids)\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids, position_ids=position_ids)\n",
    "\n",
    "    return input_embeddings, ref_input_embeddings\n",
    "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\"\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "# token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "ground_truth = 'to include, empower and support humans of all kinds'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1\n",
    "\n",
    "start_scores, end_scores = predict(input_ids, \\\n",
    "                                   position_ids=position_ids, \\\n",
    "                                   attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "\n",
    "\n",
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.roberta.embeddings)\n",
    "\n",
    "attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(position_ids, attention_mask, 0),\n",
    "                                  return_convergence_delta=True)\n",
    "attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                additional_forward_args=(position_ids, attention_mask, 1),\n",
    "                                return_convergence_delta=True)\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "attributions_start_sum = summarize_attributions(attributions_start)\n",
    "attributions_end_sum = summarize_attributions(attributions_end)\n",
    "\n",
    "# storing couple samples in an array for visualization purposes\n",
    "start_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_start_sum,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        torch.argmax(start_scores),\n",
    "                        str(ground_truth_start_ind),\n",
    "                        attributions_start_sum.sum(),\n",
    "                        all_tokens,\n",
    "                        delta_start)\n",
    "\n",
    "end_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_end_sum,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        torch.argmax(end_scores),\n",
    "                        str(ground_truth_end_ind),\n",
    "                        attributions_end_sum.sum(),\n",
    "                        all_tokens,\n",
    "                        delta_end)\n",
    "\n",
    "print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "viz.visualize_text([start_position_vis])\n",
    "\n",
    "print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "viz.visualize_text([end_position_vis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
