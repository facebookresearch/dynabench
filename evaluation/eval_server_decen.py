# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import json
import logging
import multiprocessing
import os
import sys
import time

import boto3
import requests
import uuid
from datasets.common import BaseDataset

from eval_config import eval_config
from utils.helpers import api_download_dataset, dotdict, load_models_ids_for_task_owners
from utils.logging import init_logger
from utils.requester_decen import Requester


sys.path.append("../api")  # noqa isort:skip
import common.helpers as util  # noqa isort:skip

# TODO: [BE] strong typing on all interfce methods
sleep_interval = 5
scheduler_update_interval = 300
logger = logging.getLogger("evaluation")

DYNABENCH_API = eval_config["DYNABENCH_API"]
decen_eaas_secret = eval_config["decen_eaas_secret"]


def load_datasets_for_task_owner():
    task_code = eval_config["task_code"]
    data = {"task_code": task_code}

    r = requests.get(
        f"{DYNABENCH_API}/tasks/listdatasets",
        data=json.dumps(util.wrap_data_with_signature(data, decen_eaas_secret)),
        headers={"Content-Type": "application/json"},
        verify=False,
    )

    jsonResponse = r.json()
    datasetJsonResponse = jsonResponse["datasets_metadata"]
    taskJsonResponse = jsonResponse["task_metadata"]

    datasets_dict = {}

    json_task = util.json_decode(taskJsonResponse)

    # Change s3 bucket of the task to the one specified in the eval config
    json_task["s3_bucket"] = eval_config["dataset_s3_bucket"]
    task = dotdict(json_task)

    for dataset_json in datasetJsonResponse:
        dataset = util.json_decode(dataset_json)

        rid, _ = dataset.pop("rid"), dataset.pop("tid")
        del dataset["id"], dataset["desc"]
        if dataset["name"] not in datasets_dict:
            datasets_dict[dataset["name"]] = BaseDataset(
                round_id=rid,
                task_code=task_code,
                db_connection_avail=False,
                db_connection_not_avail_task_info=task,
                **dataset,
            )

    # This means there are no datasets for the task, as
    # opposed to the request not going through
    succesful_but_empty = (r.status_code == 200) and (len(datasetJsonResponse) == 0)

    return datasets_dict, succesful_but_empty


def main():
    init_logger("evaluation")
    server_id = eval_config["eval_server_id"]
    logger.info(f"Start evaluation server '{server_id}'")
    succesful_but_empty = False

    sqs = boto3.resource(
        "sqs",
        aws_access_key_id=eval_config["aws_access_key_id"],
        aws_secret_access_key=eval_config["aws_secret_access_key"],
        region_name=eval_config["aws_region"],
    )
    queue = sqs.get_queue_by_name(QueueName=eval_config["evaluation_sqs_queue"])
    dataset_dict, succesful_but_empty = load_datasets_for_task_owner()
    active_model_ids = load_models_ids_for_task_owners()

    while (not dataset_dict) and (not succesful_but_empty):
        logger.info("Haven't got dataset_dict. Sleep.")
        time.sleep(sleep_interval)
    requester = Requester(eval_config, dataset_dict, active_model_ids)

    cpus = eval_config.get("compute_metric_processes", 2)
    with multiprocessing.pool.Pool(cpus) as pool:
        timer = scheduler_update_interval
        while True:
            try:
                # On each iteration, submit all requested jobs
                for message in queue.receive_messages():

                    msg = json.loads(message.body)

                    if msg.get("eval_server_id", "default") != server_id:
                        logger.info(
                            f"Evaluation server {server_id} ignored message {msg}"
                        )
                        continue

                    logger.info(
                        f"Evaluation server {server_id} received SQS message {msg}"
                    )

                    # load the necessary dictionary dict
                    if msg.get("reload_datasets", False):
                        succesful_but_empty = False
                        (
                            dataset_dict,
                            succesful_but_empty,
                        ) = load_datasets_for_task_owner()
                        active_model_ids = load_models_ids_for_task_owners()

                        while (not dataset_dict) and (not succesful_but_empty):
                            logger.info("Haven't got dataset_dict. Sleep.")
                            time.sleep(sleep_interval)
                        requester = Requester(
                            eval_config, dataset_dict, active_model_ids
                        )

                    # load the necessary active model ids dict
                    if msg.get("reload_models", False):
                        active_model_ids = load_models_ids_for_task_owners()
                        requester = Requester(
                            eval_config, dataset_dict, active_model_ids
                        )

                    download_dataset_message = msg.get("download_dataset", False)
                    if download_dataset_message:
                        dataset_id = msg.get("dataset_id", None)
                        delta_metric_types = msg.get("delta_metric_types", None)
                        s3_paths = msg.get("s3_paths", None)
                        dataset_name = msg.get("dataset_name", None)
                        bucket_name = eval_config["dataset_s3_bucket"]

                        if (
                            (dataset_id is None)
                            or (delta_metric_types is None)
                            or (s3_paths is None)
                        ):
                            logger.info(
                                "Invalid argument for executing download_dataset "
                                "message! Need to specify s3_paths, "
                                "delta_metric_types, dataset_id when sending download"
                            )
                        else:
                            list_perturb_to_download = [None] + delta_metric_types
                            if len(list_perturb_to_download) != len(s3_paths):
                                logger.info(
                                    "Wrong sized lists when downloading "
                                    "datasets...aborting download"
                                )

                            n = len(list_perturb_to_download)
                            for i in range(n):
                                perturb_prefix = list_perturb_to_download[i]
                                s3_path = s3_paths[i]

                                dataset_download_filename = api_download_dataset(
                                    dataset_id, perturb_prefix
                                )

                                session = boto3.Session(
                                    aws_access_key_id=eval_config["aws_access_key_id"],
                                    aws_secret_access_key=eval_config[
                                        "aws_secret_access_key"
                                    ],
                                    region_name=eval_config["aws_region"],
                                )

                                s3_client = session.client("s3")
                                response = s3_client.upload_fileobj(
                                    open(dataset_download_filename, "rb"),
                                    bucket_name,
                                    s3_path,
                                )

                                if response:
                                    logger.info(response)

                                os.remove(dataset_download_filename)

                                # add a message back to the queue to eval this dataset
                                msg = {
                                    "reload_datasets": True,
                                    "model_id": "*",
                                    "dataset_name": dataset_name,
                                    "eval_server_id": server_id,
                                }
                                queue.send_message(MessageBody=json.dumps(msg))

                    if not download_dataset_message:
                        requester.request(msg)

                    queue.delete_messages(
                        Entries=[
                            {
                                "Id": str(uuid.uuid4()),
                                "ReceiptHandle": message.receipt_handle,
                            }
                        ]
                    )
                requester.submit()

                # Update job status on scheduler interval
                if timer >= scheduler_update_interval:
                    requester.update_status()
                    timer = 0
                try:
                    # Evaluate one job
                    job = requester.computer.find_next_ready_job()
                    if job:
                        logger.info(pool)
                        # requester.computer.compute_one_async(pool, job)
                        requester.computer.compute_one_blocking(pool, job)
                except Exception as e:
                    print("job I failed on was")
                    print(job.to_dict())

                    print("dataset dict was ")
                    print(dataset_dict)

                    print("active model ids was")
                    print(active_model_ids)
                    print(e)

            except Exception as e:
                logger.info(f"Got exception while processing request: {message.body}")
                print(e)

            time.sleep(sleep_interval)
            timer += sleep_interval


if __name__ == "__main__":
    main()
