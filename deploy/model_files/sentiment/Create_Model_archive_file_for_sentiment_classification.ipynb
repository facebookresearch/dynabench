{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a model archive file to be used in the torchserve for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads the model from the s3 link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model and rename it to pytorch_model.bin, then move it to model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-30 10:21:10--  http://dl.fbaipublicfiles.com/dynabench/hs/roberta_round1.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 501003010 (478M) [application/octet-stream]\n",
      "Saving to: ‘roberta_round1.bin’\n",
      "\n",
      "roberta_round1.bin  100%[===================>] 477.79M  17.0MB/s    in 12s     \n",
      "\n",
      "2020-06-30 10:21:23 (40.2 MB/s) - ‘roberta_round1.bin’ saved [501003010/501003010]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://dl.fbaipublicfiles.com/dynabench/sentiment/roberta_round1.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installs torchserve and torch-model-archiver to be used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchserve in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch-model-archiver in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (5.4.5)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (0.18.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (5.4.1)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch-model-archiver) (0.0.3)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
      "\u001b[K    100% |████████████████████████████████| 757kB 22.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.8.0-rc4 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 13.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.20.0)\n",
      "Collecting sentencepiece (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/e5/0366f50a00db181f4b7f3bdc408fc7c4177657f5bf45cb799b79fb4ce15c/sentencepiece-0.1.92-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 24.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Collecting sacremoses (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 32.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/86/461fa59347051f378e661fcaf0a6d793450c779f4a29f338ae37c52d8575/regex-2020.6.8-cp36-cp36m-manylinux1_x86_64.whl (660kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 23.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (1.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.2.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl (298kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 45.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: tokenizers, sentencepiece, regex, joblib, sacremoses, transformers\n",
      "Successfully installed joblib-0.15.1 regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.92 tokenizers-0.8.0rc4 transformers-3.0.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchserve torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torchscript file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version 3.0.0\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "Use model and tokenizer from directory roberta-base\n"
     ]
    }
   ],
   "source": [
    "#The model should be inside the model folder,go to the model folder and execute the below command\n",
    "#python3 create_torchscript.py \n",
    "#then move the .pt file from transformer_model folder to model folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates session to get the defualt bucket name to push the tar file\n",
    "import boto3, time, json, sagemaker\n",
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "\n",
    "model_file_name = \"sent_class_r1\"\n",
    "bucket_name = \"sagemaker-us-west-1-096166425824\"\n",
    "prefix = 'torchserve'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torch-model-archiver [-h] --model-name MODEL_NAME --serialized-file\r\n",
      "                            SERIALIZED_FILE [--model-file MODEL_FILE]\r\n",
      "                            --handler HANDLER [--source-vocab SOURCE_VOCAB]\r\n",
      "                            [--extra-files EXTRA_FILES]\r\n",
      "                            [--runtime {python,python2,python3}]\r\n",
      "                            [--export-path EXPORT_PATH]\r\n",
      "                            [--archive-format {tgz,no-archive,default}] [-f]\r\n",
      "                            -v VERSION\r\n",
      "\r\n",
      "Torch Model Archiver Tool\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model-name MODEL_NAME\r\n",
      "                        Exported model name. Exported file will be named as\r\n",
      "                        model-name.mar and saved in current working directory if no --export-path is\r\n",
      "                        specified, else it will be saved under the export path\r\n",
      "  --serialized-file SERIALIZED_FILE\r\n",
      "                        Path to .pt or .pth file containing state_dict in case of eager mode\r\n",
      "                        or an executable ScriptModule in case of TorchScript.\r\n",
      "  --model-file MODEL_FILE\r\n",
      "                        Path to python file containing model architecture.\r\n",
      "                        This parameter is mandatory for eager mode models.\r\n",
      "                        The model architecture file must contain only one\r\n",
      "                        class definition extended from torch.nn.modules.\r\n",
      "  --handler HANDLER     TorchServe's default handler name\r\n",
      "                         or handler python file path to handle custom TorchServe inference logic.\r\n",
      "  --source-vocab SOURCE_VOCAB\r\n",
      "                        Vocab file for source language. Required for text based models.\r\n",
      "  --extra-files EXTRA_FILES\r\n",
      "                        Comma separated path to extra dependency files.\r\n",
      "  --runtime {python,python2,python3}\r\n",
      "                        The runtime specifies which language to run your inference code on.\r\n",
      "                        The default runtime is \"python\".\r\n",
      "  --export-path EXPORT_PATH\r\n",
      "                        Path where the exported .mar file will be saved. This is an optional\r\n",
      "                        parameter. If --export-path is not specified, the file will be saved in the\r\n",
      "                        current working directory. \r\n",
      "  --archive-format {tgz,no-archive,default}\r\n",
      "                        The format in which the model artifacts are archived.\r\n",
      "                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\r\n",
      "                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\r\n",
      "                        use this option.\r\n",
      "                        \"no-archive\": This option creates an non-archived version of model artifacts\r\n",
      "                        at \"export-path/{model-name}\" location. As a result of this choice, \r\n",
      "                        MANIFEST file will be created at \"export-path/{model-name}\" location\r\n",
      "                        without archiving these model files\r\n",
      "                        \"default\": This creates the model-archive in <model-name>.mar format.\r\n",
      "                        This is the default archiving format. Models archived in this format\r\n",
      "                        will be readily hostable on native TorchServe.\r\n",
      "  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\r\n",
      "                        name as that provided in --model-name in the path specified by --export-path\r\n",
      "                        will overwritten\r\n",
      "  -v VERSION, --version VERSION\r\n",
      "                        Model's version\r\n"
     ]
    }
   ],
   "source": [
    "# !torch-model-archiver --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure you have the following files in the model folder pytorch_model.pt,Transformer_handler_generalized.py,vocab.json,setup_config.json,special_tokens_map.json,settings.py,tokenizer_config.json,merges.txt,qa_utils.py,config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name sent_class_r1 --version 1.0 --serialized-file ./model/pytorch_model.pt --handler ./model/Transformer_handler_generalized.py --extra-files \"./model/vocab.json,./model/setup_config.json,./model/special_tokens_map.json,./model/settings.py,./model/tokenizer_config.json,./model/merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mar file will be present in the home directory\n",
    "#This creates a tar file to be used in the sagemaker deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_class_r1.mar\r\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz {model_file_name}.tar.gz {model_file_name}.mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moves the tar file to mars folder\n",
    "!mv {model_file_name mars}.tar.gz {model_file_name}.mar ./mars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: mars/sent_class_r1.tar.gz to s3://sagemaker-us-west-1-096166425824/torchserve/models/sent_class/sent_class_r1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# change the folder name before the last slash based on the task\n",
    "!aws s3 cp mars/{model_file_name}.tar.gz s3://{bucket_name}/{prefix}/models/sent_class/\n",
    "#The below s3 link will be given as the model data for sagemaker while deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
