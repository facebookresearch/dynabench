{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a model archive file to be used in the torchserve for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads the model from the s3 link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-30 10:23:14--  http://dl.fbaipublicfiles.com/dynabench/hs/roberta_round1.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 501003010 (478M) [application/octet-stream]\n",
      "Saving to: ‘roberta_round1.bin’\n",
      "\n",
      "roberta_round1.bin  100%[===================>] 477.79M  74.9MB/s    in 6.2s    \n",
      "\n",
      "2020-06-30 10:23:21 (76.8 MB/s) - ‘roberta_round1.bin’ saved [501003010/501003010]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the model and rename it to pytorch_model.bin, then move it to model folder\n",
    "!wget http://dl.fbaipublicfiles.com/dynabench/hs/roberta_round1.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installs torchserve and torch-model-archiver to be used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchserve in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch-model-archiver in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (0.18.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (5.4.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchserve) (5.4.5)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch-model-archiver) (0.0.3)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchserve torch-model-archiver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torchscript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model should be inside the model folder,go to the model folder and execute the below command\n",
    "#python3 create_torchscript.py \n",
    "#then move the .pt file from transformer_model folder to model folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates session to get the defualt bucket name to push the tar file\n",
    "import boto3, time, json, sagemaker\n",
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "\n",
    "model_file_name = \"hs_r1\"\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torch-model-archiver [-h] --model-name MODEL_NAME --serialized-file\r\n",
      "                            SERIALIZED_FILE [--model-file MODEL_FILE]\r\n",
      "                            --handler HANDLER [--source-vocab SOURCE_VOCAB]\r\n",
      "                            [--extra-files EXTRA_FILES]\r\n",
      "                            [--runtime {python,python2,python3}]\r\n",
      "                            [--export-path EXPORT_PATH]\r\n",
      "                            [--archive-format {tgz,no-archive,default}] [-f]\r\n",
      "                            -v VERSION\r\n",
      "\r\n",
      "Torch Model Archiver Tool\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model-name MODEL_NAME\r\n",
      "                        Exported model name. Exported file will be named as\r\n",
      "                        model-name.mar and saved in current working directory if no --export-path is\r\n",
      "                        specified, else it will be saved under the export path\r\n",
      "  --serialized-file SERIALIZED_FILE\r\n",
      "                        Path to .pt or .pth file containing state_dict in case of eager mode\r\n",
      "                        or an executable ScriptModule in case of TorchScript.\r\n",
      "  --model-file MODEL_FILE\r\n",
      "                        Path to python file containing model architecture.\r\n",
      "                        This parameter is mandatory for eager mode models.\r\n",
      "                        The model architecture file must contain only one\r\n",
      "                        class definition extended from torch.nn.modules.\r\n",
      "  --handler HANDLER     TorchServe's default handler name\r\n",
      "                         or handler python file path to handle custom TorchServe inference logic.\r\n",
      "  --source-vocab SOURCE_VOCAB\r\n",
      "                        Vocab file for source language. Required for text based models.\r\n",
      "  --extra-files EXTRA_FILES\r\n",
      "                        Comma separated path to extra dependency files.\r\n",
      "  --runtime {python,python2,python3}\r\n",
      "                        The runtime specifies which language to run your inference code on.\r\n",
      "                        The default runtime is \"python\".\r\n",
      "  --export-path EXPORT_PATH\r\n",
      "                        Path where the exported .mar file will be saved. This is an optional\r\n",
      "                        parameter. If --export-path is not specified, the file will be saved in the\r\n",
      "                        current working directory. \r\n",
      "  --archive-format {tgz,no-archive,default}\r\n",
      "                        The format in which the model artifacts are archived.\r\n",
      "                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\r\n",
      "                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\r\n",
      "                        use this option.\r\n",
      "                        \"no-archive\": This option creates an non-archived version of model artifacts\r\n",
      "                        at \"export-path/{model-name}\" location. As a result of this choice, \r\n",
      "                        MANIFEST file will be created at \"export-path/{model-name}\" location\r\n",
      "                        without archiving these model files\r\n",
      "                        \"default\": This creates the model-archive in <model-name>.mar format.\r\n",
      "                        This is the default archiving format. Models archived in this format\r\n",
      "                        will be readily hostable on native TorchServe.\r\n",
      "  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\r\n",
      "                        name as that provided in --model-name in the path specified by --export-path\r\n",
      "                        will overwritten\r\n",
      "  -v VERSION, --version VERSION\r\n",
      "                        Model's version\r\n"
     ]
    }
   ],
   "source": [
    "# !torch-model-archiver --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure you have the following files in the model folder pytorch_model.pt,Transformer_handler_generalized.py,vocab.json,setup_config.json,special_tokens_map.json,settings.py,tokenizer_config.json,merges.txt,qa_utils.py,config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name hs_r1 --version 1.0 --serialized-file ./model/pytorch_model.pt --handler ./model/Transformer_handler_generalized.py --extra-files \"./model/vocab.json,./model/setup_config.json,./model/special_tokens_map.json,./model/settings.py,./model/tokenizer_config.json,./model/merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mar file will be present in the home directory\n",
    "#This creates a tar file to be used in the sagemaker deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs_r1.mar\r\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz {model_file_name}.tar.gz {model_file_name}.mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moves the tar file to mars folder\n",
    "!mv {model_file_name mars}.tar.gz {model_file_name}.mar ./mars/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copies the model from mars folder into the hatespeech folder of s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: mars/hs_r1.tar.gz to s3://sagemaker-us-west-1-096166425824/torchserve/models/hs/hs_r1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# change the folder name before the last slash based on the task\n",
    "!aws s3 cp mars/{model_file_name}.tar.gz s3://{bucket_name}/{prefix}/models/hs/\n",
    "#The below s3 link will be given as the model data for sagemaker while deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
